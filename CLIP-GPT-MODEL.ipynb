{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c45e70c7-6703-480a-a0b4-10e6296315b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU Name: GRID A100X-10C\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55857d8d-cc28-4c75-be55-6cc772cf1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch torchvision evaluate pycocoevalcap scikit-learn matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecbf4c-76d5-4736-a361-c456225dfe4f",
   "metadata": {},
   "source": [
    "Step 2: Load 5k Images from MSCOCO (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eed40d-d149-47af-ae04-2bfc42ecd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets --upgrade\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b368d022-8c90-41e9-8256-96f5be7bb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create destination directory\n",
    "data_dir = r\"deepreel/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download val2017.zip\n",
    "val_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "val_zip_path = os.path.join(data_dir, \"val2017.zip\")\n",
    "urllib.request.urlretrieve(val_url, val_zip_path)\n",
    "\n",
    "# Extract val2017.zip\n",
    "with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "# Download annotations_trainval2017.zip\n",
    "ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "ann_zip_path = os.path.join(data_dir, \"annotations_trainval2017.zip\")\n",
    "urllib.request.urlretrieve(ann_url, ann_zip_path)\n",
    "\n",
    "# Extract annotations zip\n",
    "with zipfile.ZipFile(ann_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c013df32-d5c4-4d81-a269-1b39000d8fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image file: 000000397133.jpg\n",
      "Captions:\n",
      "['A man is in a kitchen making pizzas.', 'Man in apron standing on front of oven with pans and bakeware', 'A baker is working in the kitchen rolling dough.', 'A person standing by a stove in a kitchen.', 'A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "data_dir = Path(\"deepreel/data\")\n",
    "images_dir = data_dir / \"val2017\"\n",
    "annotations_file = data_dir / \"annotations\" / \"captions_val2017.json\"\n",
    "\n",
    "# Load COCO caption annotations\n",
    "with open(annotations_file, 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "# Map image_id ->  filename\n",
    "id_to_filename = {img['id']: img['file_name'] for img in captions_data['images']}\n",
    "\n",
    "# Map image_id -> all its captions\n",
    "id_to_captions = defaultdict(list)\n",
    "for ann in captions_data['annotations']:\n",
    "    id_to_captions[ann['image_id']].append(ann['caption'])\n",
    "\n",
    "image_id = next(iter(id_to_filename))\n",
    "print(f\"Image file: {id_to_filename[image_id]}\")\n",
    "print(\"Captions:\")\n",
    "print(id_to_captions[image_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "533cefb7-5283-40de-99f6-7ad564168fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load pretrained CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").eval().cuda()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.get_image_features(**inputs)\n",
    "    return embedding.squeeze(0)  # [512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aaced83d-22b9-4b89-9e76-5b499e571926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_caption(caption, max_length=32):\n",
    "    tokens = tokenizer(\n",
    "        caption,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens.input_ids.squeeze(0)  # [max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "380daf22-a36e-43ac-a22f-3a1bc0a792b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class CocoGPTDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_ids, id_to_captions, tokenizer, max_length=32):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_ids = image_ids\n",
    "        self.id_to_captions = id_to_captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        image_path = self.image_paths[idx]\n",
    "    \n",
    "        # Get a random caption\n",
    "        raw_caption = random.choice(self.id_to_captions[img_id])\n",
    "    \n",
    "        prompt = \"An image of \"\n",
    "        full_caption = prompt + raw_caption\n",
    "    \n",
    "        # Get image features\n",
    "        img_embedding = get_image_embedding(image_path)  \n",
    "    \n",
    "        # Tokenize with padding and truncation\n",
    "        tokens = self.tokenizer.encode(\n",
    "            full_caption,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).squeeze(0)  # shape: [max_length]\n",
    "    \n",
    "        return img_embedding, tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "742f0f14-b6dd-42f0-846e-d2195700e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(id_to_filename.keys())[:1250]\n",
    "dataset = CocoGPTDataset(image_ids, images_dir, id_to_filename, id_to_captions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae45e248-0649-47f5-8aa5-703c3500ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageCaptioningGPT2(nn.Module):\n",
    "    def __init__(self, gpt_model_name=\"gpt2\", embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n",
    "        self.gpt.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.map_img_to_gpt = nn.Linear(embed_dim, self.gpt.config.n_embd)\n",
    "        self.tokenizer_pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def forward(self, img_embedding, input_ids, labels=None):\n",
    "\n",
    "        # Map image -> GPT2 embedding space\n",
    "        img_token = self.map_img_to_gpt(img_embedding).unsqueeze(1)  \n",
    "        tok_embeds = self.gpt.transformer.wte(input_ids)  \n",
    "\n",
    "        # Concatenate image token at start\n",
    "        input_embeds = torch.cat([img_token, tok_embeds], dim=1) \n",
    "\n",
    "        # Shift labels right to align with logits\n",
    "        if labels is not None:\n",
    "            labels = torch.cat([\n",
    "                torch.full((labels.shape[0], 1), -100).to(labels.device),\n",
    "                labels\n",
    "            ], dim=1)\n",
    "\n",
    "        output = self.gpt(inputs_embeds=input_embeds, labels=labels)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7122adf-163a-4ba3-aed0-aac75c73f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Dataset ready with\", len(dataset), \"samples.\")\n",
    "\n",
    "model = ImageCaptioningGPT2().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ea80f-ec36-4e18-8d98-75308fb7f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for img_feats, cap_tokens in loader:\n",
    "        img_feats = img_feats.cuda()\n",
    "        cap_tokens = cap_tokens.cuda()\n",
    "\n",
    "        outputs = model(img_feats, cap_tokens, labels=cap_tokens)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66bdc7-55a0-48c3-a2b6-ac4dd20072d9",
   "metadata": {},
   "source": [
    "Step 10: Prepare Validation Set for Inference. Let’s reuse 200 images (e.g. last 200 of your dataset):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6435514-5f69-4640-9e6e-aa3c750ff5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = image_ids[-200:]\n",
    "val_dataset = CocoGPTDataset(val_ids, images_dir, id_to_filename, id_to_captions)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdffd64-86b2-485a-ad43-1d621c807d2b",
   "metadata": {},
   "source": [
    "Step 11: Generate Captions from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7b58ea10-d3d5-4e76-acaf-8910cb97af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_embedding, tokenizer, max_length=32, num_beams=5):\n",
    "    model.eval()\n",
    "\n",
    "    img_embedding = img_embedding.cuda().unsqueeze(0) \n",
    "\n",
    "    # 1. Get the prompt text and its embeddings\n",
    "    prompt_text = \"An image of\"\n",
    "    prompt_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").cuda()  \n",
    "    prompt_embeds = model.gpt.transformer.wte(prompt_ids)  \n",
    "\n",
    "    # 2. Project the image and prepend as token\n",
    "    img_token = model.map_img_to_gpt(img_embedding).unsqueeze(1)  \n",
    "\n",
    "    # 3. Combine image token + prompt embeddings\n",
    "    input_embeds = torch.cat([img_token, prompt_embeds], dim=1) \n",
    "\n",
    "    # 4. Generate from this combined context\n",
    "    generated_ids = model.gpt.generate(\n",
    "        inputs_embeds=input_embeds,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    # 5. Decode and return\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde23dd-f0ca-4b3e-90c3-b1a776fb8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking sample predictions:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    img_id = val_ids[i]\n",
    "    img_path = images_dir / id_to_filename[img_id]\n",
    "    img_embedding = get_image_embedding(img_path)\n",
    "    caption = generate_caption(model, img_embedding, tokenizer)\n",
    "\n",
    "    print(f\"Image ID: {img_id}\")\n",
    "    print(\"Predicted:\", generate_caption(model, img_embedding, tokenizer))\n",
    "    print(\"Reference:\", refs[i][:2]) \n",
    "    print(\"—\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6c2d10a5-f6a5-48e0-b019-82c046b5d1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: nltk in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: pycocoevalcap in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (1.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: dill in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (0.28.0)\n",
      "Requirement already satisfied: packaging in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: click in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from pycocoevalcap) (2.0.8)\n",
      "Requirement already satisfied: filelock in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (19.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/exouser/.conda/envs/csci-5922/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate nltk pycocoevalcap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c5b82-7494-481d-ad11-bcd7b3c9a046",
   "metadata": {},
   "source": [
    "Evaluate with BLEU & CIDEr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d767b4-0903-47c1-a9da-f6b2427db328",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tylin/coco-caption\n",
    "!pip install -e coco-caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "62c22bbc-1cb2-457d-9d54-e27bccf4dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "refs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6f591e6f-aea9-47c2-be4a-c48e4e614ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_id in val_ids:\n",
    "    img_path = images_dir / id_to_filename[img_id]\n",
    "    img_embedding = get_image_embedding(img_path)\n",
    "\n",
    "    # Generate predicted caption\n",
    "    generated_caption = generate_caption(model, img_embedding, tokenizer)\n",
    "    preds.append(generated_caption)\n",
    "\n",
    "    # Append reference captions (COCO allows multiple per image)\n",
    "    refs.append(id_to_captions[img_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f1fe657a-cfa3-4e5e-950d-b63b204ffc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "results = [\n",
    "    {\"image_id\": int(img_id), \"caption\": caption}\n",
    "    for img_id, caption in zip(val_ids, preds)\n",
    "]\n",
    "\n",
    "# Save references\n",
    "from collections import defaultdict\n",
    "gts = defaultdict(list)\n",
    "for img_id, captions in zip(val_ids, refs):\n",
    "    for cap in captions:\n",
    "        gts[int(img_id)].append({\"caption\": cap})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3602da-212d-48d8-b745-9c916f66e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf coco-caption\n",
    "!git clone https://github.com/tylin/coco-caption.git\n",
    "\n",
    "with open(\"coco-caption/setup.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "from setuptools import setup, find_packages\n",
    "setup(\n",
    "    name='coco-caption',\n",
    "    version='1.0',\n",
    "    packages=find_packages(),\n",
    "    package_dir={'': '.'},\n",
    ")\n",
    "\"\"\")\n",
    "!pip install -e coco-caption\n",
    "!2to3 -w coco-caption/pycocoevalcap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2bf8e052-e781-42f8-9313-43539acbe11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "# Monkey-patch the evaluate function to exclude METEOR\n",
    "def patched_evaluate(self):\n",
    "    imgIds = self.params['image_id']\n",
    "    gts = {}\n",
    "    res = {}\n",
    "    for imgId in imgIds:\n",
    "        gts[imgId] = self.coco.imgToAnns[imgId]\n",
    "        res[imgId] = self.cocoRes.imgToAnns[imgId]\n",
    "\n",
    "    from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "    tokenizer = PTBTokenizer()\n",
    "    gts = tokenizer.tokenize(gts)\n",
    "    res = tokenizer.tokenize(res)\n",
    "\n",
    "    # Set up scorers (no METEOR)\n",
    "    from pycocoevalcap.bleu.bleu import Bleu\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "        (Cider(), \"CIDEr\"),\n",
    "    ]\n",
    "\n",
    "    for scorer, method in scorers:\n",
    "        print(f\"computing {scorer.method()} score...\")\n",
    "        score, scores = scorer.compute_score(gts, res)\n",
    "        if type(method) == list:\n",
    "            for sc, scs, m in zip(score, scores, method):\n",
    "                self.setEval(sc, m)\n",
    "                self.setImgToEvalImgs(scs, gts.keys(), m)\n",
    "                print(f\"{m}: {sc:.3f}\")\n",
    "        else:\n",
    "            self.setEval(score, method)\n",
    "            self.setImgToEvalImgs(scores, gts.keys(), method)\n",
    "            print(f\"{method}: {score:.3f}\")\n",
    "    self.setEvalImgs()\n",
    "\n",
    "COCOEvalCap.evaluate = patched_evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "67853c16-d72a-48dc-b113-2ef470929891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import tempfile, json\n",
    "\n",
    "# Step 1: Normalize image IDs to int (very important)\n",
    "results_fixed = [{\"image_id\": int(r[\"image_id\"]), \"caption\": r[\"caption\"]} for r in results]\n",
    "\n",
    "# Step 2: Wrap references in COCO-style dicts\n",
    "gts_dict = defaultdict(list)\n",
    "for img_id, cap_list in zip(val_ids, refs):\n",
    "    for cap in cap_list:\n",
    "        gts_dict[int(img_id)].append({\"caption\": cap})\n",
    "\n",
    "# Step 3: Save both to JSON files for pycocoevalcap\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".json\") as gt_file:\n",
    "    gt_data = {\n",
    "        \"images\": [{\"id\": int(k)} for k in gts_dict.keys()],\n",
    "        \"annotations\": [\n",
    "            {\"image_id\": int(k), \"id\": i, \"caption\": d[\"caption\"]}\n",
    "            for i, (k, v) in enumerate(gts_dict.items())\n",
    "            for d in v\n",
    "        ],\n",
    "        \"type\": \"captions\",\n",
    "        \"info\": \"evaluation\"\n",
    "    }\n",
    "    json.dump(gt_data, gt_file)\n",
    "    gt_path = gt_file.name\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".json\") as pred_file:\n",
    "    json.dump(results_fixed, pred_file)\n",
    "    pred_path = pred_file.name\n",
    "\n",
    "# Step 4: Monkey-patch tokenizer (to avoid Java)\n",
    "import pycocoevalcap.tokenizer.ptbtokenizer as pt_mod\n",
    "import re, string\n",
    "\n",
    "def simple_tokenize(self, captions_for_image):\n",
    "    final = {}\n",
    "    for img_id, caps in captions_for_image.items():\n",
    "        toks = []\n",
    "        for c in caps:\n",
    "            text = c['caption'].lower()\n",
    "            text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "            toks.append(\" \".join(text.split()))\n",
    "        final[img_id] = toks\n",
    "    return final\n",
    "\n",
    "pt_mod.PTBTokenizer.tokenize = simple_tokenize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2b811-52b3-4422-a809-b98105ef68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run COCOEval\n",
    "coco = COCO(gt_path)\n",
    "cocoRes = coco.loadRes(pred_path)\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "cocoEval.evaluate()\n",
    "\n",
    "# Print scores\n",
    "for metric, score in cocoEval.eval.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78c90d-e646-4d4f-b4c9-0c52b23b5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = list(cocoEval.eval.keys())\n",
    "scores = list(cocoEval.eval.values())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(metrics, scores)\n",
    "plt.title(\"Evaluation Metrics for DeepReel GPT Captioning\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527fadb-7f6e-416d-949b-00be8ecf81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample result:\", results[0])\n",
    "print(\"Sample refs:\", refs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "eb479d90-54df-4c8e-9e9d-f2a2b620d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate(dataset_fraction, image_ids, image_paths, image_id_to_captions, tokenizer):\n",
    "    print(f\"\\n Training with {int(dataset_fraction * 100)}% of dataset\")\n",
    "\n",
    "    # Sample a subset\n",
    "    subset_size = int(len(image_ids) * dataset_fraction)\n",
    "    selected_ids = image_ids[:subset_size]\n",
    "    selected_paths = image_paths[:subset_size]\n",
    "    print(f\" Selected {subset_size} samples\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    print(\"Creating dataset and dataloader\")\n",
    "    dataset = CocoGPTDataset(selected_paths, selected_ids, image_id_to_captions, tokenizer)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    print(f\" DataLoader ready with {len(dataset)} samples\")\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    print(\" Initializing model...\")\n",
    "    model = ImageCaptioningGPT2().cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train\n",
    "    print(\" Starting training...\")\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        for img_feats, cap_tokens in loader:\n",
    "            img_feats, cap_tokens = img_feats.cuda(), cap_tokens.cuda()\n",
    "            outputs = model(img_feats, cap_tokens, labels=cap_tokens)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch+1}/20 - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Generate captions\n",
    "    print(\" Generating predictions\")\n",
    "    preds, refs, val_ids = [], [], []\n",
    "    for img_id, img_path in zip(selected_ids[:100], selected_paths[:100]):\n",
    "        img_embedding = get_image_embedding(img_path)\n",
    "        caption = generate_caption(model, img_embedding, tokenizer)\n",
    "        preds.append(caption)\n",
    "        refs.append(image_id_to_captions[img_id][:5])  \n",
    "        val_ids.append(int(img_id))\n",
    "    print(\"Caption generation complete\")\n",
    "\n",
    "    # Format predictions for COCOEval\n",
    "    print(\"Formatting results for COCOEval\")\n",
    "    results = [{\"image_id\": int(iid), \"caption\": c} for iid, c in zip(val_ids, preds)]\n",
    "    gts = defaultdict(list)\n",
    "    for img_id, caps in zip(val_ids, refs):\n",
    "        for c in caps:\n",
    "            gts[img_id].append({\"caption\": c})\n",
    "\n",
    "    print(\"Patching tokenizer...\")\n",
    "    import pycocoevalcap.tokenizer.ptbtokenizer as pt_mod\n",
    "    def simple_tokenize(self, captions_for_image):\n",
    "        import re, string\n",
    "        final = {}\n",
    "        for img_id, caps in captions_for_image.items():\n",
    "            toks = []\n",
    "            for c in caps:\n",
    "                text = c['caption'].lower()\n",
    "                text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "                toks.append(\" \".join(text.split()))\n",
    "            final[img_id] = toks\n",
    "        return final\n",
    "    pt_mod.PTBTokenizer.tokenize = simple_tokenize\n",
    "\n",
    "    # Save to temp files\n",
    "    import tempfile, json\n",
    "    print(\"Writing temp JSON files...\")\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".json\") as gt_file:\n",
    "        gt_data = {\n",
    "            \"images\": [{\"id\": iid} for iid in val_ids],\n",
    "            \"annotations\": [\n",
    "                {\"image_id\": iid, \"id\": i, \"caption\": cap}\n",
    "                for i, (iid, caps) in enumerate(zip(val_ids, refs))\n",
    "                for cap in caps\n",
    "            ],\n",
    "            \"type\": \"captions\",\n",
    "            \"info\": \"generated\"\n",
    "        }\n",
    "        json.dump(gt_data, gt_file)\n",
    "        gt_path = gt_file.name\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".json\") as pred_file:\n",
    "        json.dump(results, pred_file)\n",
    "        pred_path = pred_file.name\n",
    "\n",
    "    # Run COCOEval\n",
    "    print(\"Running COCO Evaluation\")\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocoevalcap.eval import COCOEvalCap\n",
    "    coco = COCO(gt_path)\n",
    "    cocoRes = coco.loadRes(pred_path)\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    cocoEval.evaluate()\n",
    "\n",
    "    print(\" Evaluation complete.\")\n",
    "    return {metric: float(score) for metric, score in cocoEval.eval.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5a7d1c13-74c9-4ff3-8287-61806b823591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required setup\n",
    "image_ids = list(id_to_filename.keys())\n",
    "image_paths = [images_dir / id_to_filename[img_id] for img_id in image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8db33-ab51-4eea-840e-34e84a907810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the model without distillation\n",
    "sizes = [0.50]\n",
    "all_results = {}\n",
    "\n",
    "for size in sizes:\n",
    "    scores = train_and_evaluate(size, image_ids, image_paths, id_to_captions, tokenizer)\n",
    "    all_results[f\"{int(size * 100)}%\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ababc-8740-45e0-94e0-74d5db2f2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_str = list(all_results.keys())\n",
    "metrics = [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\", \"CIDEr\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    values = [all_results[size][metric] for size in sizes_str]\n",
    "    plt.figure()\n",
    "    plt.plot(sizes_str, values, marker='o')\n",
    "    plt.title(f\"{metric} vs Dataset Size\")\n",
    "    plt.xlabel(\"Dataset Size\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "756ca7f9-bfb9-477e-99d6-873a1491d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_distill(image_ids, percentage):\n",
    "    \"\"\"\n",
    "    Randomly select a subset of image_ids based on percentage.\n",
    "    \"\"\"\n",
    "    k = int(len(image_ids) * percentage)\n",
    "    return random.sample(image_ids, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f4d32ad-5f49-47d8-83b5-d32ba778602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_filepath = {img_id: path for img_id, path in zip(image_ids, image_paths)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecc797-567c-4d78-8cf0-839af0c5c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ids = random_distill(image_ids, 0.25)\n",
    "print(\"Sampled:\", len(sampled_ids), \"out of\", len(image_ids))\n",
    "print(\"Example:\", sampled_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f2e71ca-7e11-4063-ac91-b707e8062c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_and_evaluate_distilled(selected_ids, id_to_filepath, id_to_captions, tokenizer, model_label=\"\"):\n",
    "    print(f\"\\n Training on distilled subset: {len(selected_ids)} images\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Build paths from selected IDs\n",
    "    selected_paths = [id_to_filepath[img_id] for img_id in selected_ids]\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = CocoGPTDataset(selected_paths, selected_ids, id_to_captions, tokenizer)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    print(f\" Model: {model_label} | Samples: {len(dataset)}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = ImageCaptioningGPT2().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        for img_feats, cap_tokens in loader:\n",
    "            img_feats, cap_tokens = img_feats.cuda(), cap_tokens.cuda()\n",
    "            outputs = model(img_feats, cap_tokens, labels=cap_tokens)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\" Epoch {epoch+1}/20 - Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "    # Generate predictions for eval\n",
    "        print(\"Generating predictions for 100 validation images...\")\n",
    "        preds, refs, val_ids = [], [], []\n",
    "        start_infer = time.time()\n",
    "        \n",
    "        for img_id in selected_ids[:100]:\n",
    "            img_path = id_to_filepath[img_id]\n",
    "            img_embedding = get_image_embedding(img_path)\n",
    "            caption = generate_caption(model, img_embedding, tokenizer)\n",
    "            preds.append(caption)\n",
    "            refs.append(id_to_captions[img_id][:5])\n",
    "            val_ids.append(int(img_id))\n",
    "\n",
    "        total_infer_time = time.time() - start_infer\n",
    "        avg_infer_time = total_infer_time / len(preds)\n",
    "        print(f\" Inference time per caption: {avg_infer_time:.4f} seconds\")\n",
    "\n",
    "    # Format for COCOEval\n",
    "    from collections import defaultdict\n",
    "    results = [{\"image_id\": int(iid), \"caption\": c} for iid, c in zip(val_ids, preds)]\n",
    "    gts = defaultdict(list)\n",
    "    for img_id, caps in zip(val_ids, refs):\n",
    "        for c in caps:\n",
    "            gts[img_id].append({\"caption\": c})\n",
    "\n",
    "    # Tokenizer patch\n",
    "    import pycocoevalcap.tokenizer.ptbtokenizer as pt_mod\n",
    "    def simple_tokenize(self, captions_for_image):\n",
    "        import re, string\n",
    "        final = {}\n",
    "        for img_id, caps in captions_for_image.items():\n",
    "            toks = []\n",
    "            for c in caps:\n",
    "                text = c['caption'].lower()\n",
    "                text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "                toks.append(\" \".join(text.split()))\n",
    "            final[img_id] = toks\n",
    "        return final\n",
    "    pt_mod.PTBTokenizer.tokenize = simple_tokenize\n",
    "\n",
    "    # Save temporary prediction/reference files\n",
    "    import tempfile, json\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".json\") as gt_file:\n",
    "        gt_data = {\n",
    "            \"images\": [{\"id\": iid} for iid in val_ids],\n",
    "            \"annotations\": [\n",
    "                {\"image_id\": iid, \"id\": i, \"caption\": cap}\n",
    "                for i, (iid, caps) in enumerate(zip(val_ids, refs))\n",
    "                for cap in caps\n",
    "            ],\n",
    "            \"type\": \"captions\", \"info\": \"distilled\"\n",
    "        }\n",
    "        json.dump(gt_data, gt_file)\n",
    "        gt_path = gt_file.name\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".json\") as pred_file:\n",
    "        json.dump(results, pred_file)\n",
    "        pred_path = pred_file.name\n",
    "\n",
    "    # Evaluate\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocoevalcap.eval import COCOEvalCap\n",
    "    coco = COCO(gt_path)\n",
    "    cocoRes = coco.loadRes(pred_path)\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    cocoEval.evaluate()\n",
    "\n",
    "    print(\" Evaluation complete.\")\n",
    "    total_time = time.time() - start_time\n",
    "    return (\n",
    "    {metric: float(score) for metric, score in cocoEval.eval.items()},\n",
    "    total_time,\n",
    "    avg_infer_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b81f3a-c3ba-4340-ab03-ee6d7ea64e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores_random = {}\n",
    "all_times_random = {}\n",
    "all_infer_times_random = {}\n",
    "fractions = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "\n",
    "for frac in fractions:\n",
    "    print(f\"\\n RANDOM SAMPLING | Dataset: {int(frac * 100)}%\")\n",
    "    sampled_ids = random_distill(image_ids, frac)\n",
    "    \n",
    "    scores, duration, infer_time = train_and_evaluate_distilled(\n",
    "        selected_ids=sampled_ids,\n",
    "        id_to_filepath=id_to_filepath,\n",
    "        id_to_captions=id_to_captions,\n",
    "        tokenizer=tokenizer,\n",
    "        model_label=f\"Random - {int(frac * 100)}%\"\n",
    "    )\n",
    "\n",
    "    all_scores_random[int(frac * 100)] = scores\n",
    "    all_times_random[int(frac * 100)] = duration\n",
    "    all_infer_times_random[int(frac * 100)] = infer_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fef15-9912-4ad2-9513-69e6fe59793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\", \"CIDEr\"]\n",
    "x_vals = sorted(all_scores_random.keys())\n",
    "\n",
    "for metric in metrics:\n",
    "    y_vals = [all_scores_random[size][metric] for size in x_vals]\n",
    "    plt.figure()\n",
    "    plt.plot(x_vals, y_vals, marker='o', label='Random Distillation')\n",
    "    plt.title(f\"{metric} vs Dataset Size (Random)\")\n",
    "    plt.xlabel(\"Dataset Size (%)\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(x_vals)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aef2880-6f1e-448f-a47c-f24d4f032d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def gradient_based_distill(image_ids, image_paths, id_to_captions, tokenizer, percentage):\n",
    "    \"\"\"\n",
    "    Select images with highest per-sample gradient norm using a small probe model.\n",
    "    \"\"\"\n",
    "    print(\"Starting true gradient-based scoring (via gradient norms)...\")\n",
    "\n",
    "    # Build image paths\n",
    "    id_to_path = {img_id: path for img_id, path in zip(image_ids, image_paths)}\n",
    "    selected_paths = [id_to_path[img_id] for img_id in image_ids]\n",
    "\n",
    "    dataset = CocoGPTDataset(selected_paths, image_ids, id_to_captions, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = ImageCaptioningGPT2().cuda()\n",
    "    model.train() \n",
    "\n",
    "    id_gradnorm = []\n",
    "\n",
    "    for (img_feat, cap_token), img_id in zip(loader, image_ids):\n",
    "        img_feat, cap_token = img_feat.cuda(), cap_token.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model(img_feat, cap_token, labels=cap_token)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "\n",
    "        id_gradnorm.append((img_id, total_norm))\n",
    "\n",
    "    id_gradnorm.sort(key=lambda x: x[1], reverse=True)\n",
    "    k = int(len(image_ids) * percentage)\n",
    "    top_ids = [img_id for img_id, _ in id_gradnorm[:k]]\n",
    "\n",
    "    print(f\" Selected top {k} samples with highest gradient norms.\")\n",
    "    return top_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12dbd52-c70f-4a6e-91be-5a05aa2e56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_gradient = {}\n",
    "all_times_gradient = {}\n",
    "all_infer_times_gradient = {}\n",
    "fractions = [0.5, 0.75, 1.0]\n",
    "\n",
    "for frac in fractions:\n",
    "    print(f\"\\n GRADIENT-BASED | Dataset: {int(frac * 100)}%\")\n",
    "    sampled_ids = gradient_based_distill(image_ids, image_paths, id_to_captions, tokenizer, frac)\n",
    "\n",
    "    scores, duration, infer_time = train_and_evaluate_distilled(\n",
    "        selected_ids=sampled_ids,\n",
    "        id_to_filepath=id_to_filepath,\n",
    "        id_to_captions=id_to_captions,\n",
    "        tokenizer=tokenizer,\n",
    "        model_label=f\"Gradient-{int(frac * 100)}%\"\n",
    "    )\n",
    "\n",
    "    all_scores_gradient[int(frac * 100)] = scores\n",
    "    all_times_gradient[int(frac * 100)] = duration\n",
    "    all_infer_times_gradient[int(frac * 100)] = infer_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb3bb3-76d0-49bb-a8b5-f460d8756696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\", \"CIDEr\"]\n",
    "sizes = sorted(all_scores_random.keys())  # [25, 50, 75, 100]\n",
    "\n",
    "for metric in metrics:\n",
    "    y_random = [all_scores_random[size][metric] for size in sizes]\n",
    "    y_gradient = [all_scores_gradient[size][metric] for size in sizes]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(sizes, y_random, marker='o', label='Random')\n",
    "    plt.plot(sizes, y_gradient, marker='o', label='Gradient-Based')\n",
    "    plt.title(f\"{metric} vs Dataset Size\")\n",
    "    plt.xlabel(\"Dataset Size (%)\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(sizes)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b68f2-394e-4784-9972-7d877e2137f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sizes = sorted(all_times_random.keys())\n",
    "time_random = [all_times_random[size] for size in sizes]\n",
    "time_gradient = [all_times_gradient[size] for size in sizes]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sizes, time_random, marker='o', label=\"Random\")\n",
    "plt.plot(sizes, time_gradient, marker='o', label=\"Gradient-Based\")\n",
    "plt.title(\"Training Time vs Dataset Size\")\n",
    "plt.xlabel(\"Dataset Size (%)\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.xticks(sizes)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03c110-b3c2-4e82-b926-d95075ab9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = sorted(all_infer_times_random.keys())\n",
    "infer_random = [all_infer_times_random[size] for size in sizes]\n",
    "infer_gradient = [all_infer_times_gradient[size] for size in sizes]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sizes, infer_random, marker='o', label=\"Random\")\n",
    "plt.plot(sizes, infer_gradient, marker='o', label=\"Gradient-Based\")\n",
    "plt.title(\"Inference Time per Caption vs Dataset Size\")\n",
    "plt.xlabel(\"Dataset Size (%)\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.xticks(sizes)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7318c90-1b0c-4692-99c9-a40a619032bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), \"trained_gpt_captioner.pth\")\n",
    "print(\"Model saved as 'trained_gpt_captioner.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01144ba5-5f0c-4100-9d90-accdaafb5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ImageCaptioningGPT2()\n",
    "model.load_state_dict(torch.load(\"trained_gpt_captioner.pth\"))\n",
    "model = model.cuda()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89b10134-2d4e-4881-8686-5e56a049e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption2(model, img_embedding, tokenizer, max_length=32):\n",
    "    model.eval()\n",
    "    device = img_embedding.device\n",
    "\n",
    "    # Convert image embedding to GPT2 token space\n",
    "    img_token = model.map_img_to_gpt(img_embedding).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    # Use a textual prompt to help guide generation\n",
    "    prompt = \"An image of\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prompt_embeds = model.gpt.transformer.wte(input_ids)\n",
    "\n",
    "    # Concatenate image token with prompt\n",
    "    input_embeds = torch.cat([img_token, prompt_embeds], dim=1)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.gpt.generate(\n",
    "            inputs_embeds=input_embeds,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad541383-f02b-42d4-937d-3541ca5ce5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), \"trained_gpt_captioner01.pth\")\n",
    "print(\"Model saved as 'trained_gpt_captioner01.pth'\")\n",
    "model = ImageCaptioningGPT2()\n",
    "model.load_state_dict(torch.load(\"trained_gpt_captioner01.pth\"))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "caption = generate_caption2(model, get_image_embedding(\"test.jpg\"), tokenizer)\n",
    "print(\"Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985c859-cb06-4b76-8238-2aee42c7d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = generate_caption2(model, get_image_embedding(\"test2.jpg\"), tokenizer)\n",
    "print(\"Caption:\", caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (csci-5922)",
   "language": "python",
   "name": "csci-5922"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
