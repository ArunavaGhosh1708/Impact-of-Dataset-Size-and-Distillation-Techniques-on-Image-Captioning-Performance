{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of Dataset Size and Distillation Techniques on Image Captioning Performance: An Empirical Study\n",
    "\n",
    "Authors: Srushti Sangawar and Arunava Ghosh\n",
    "\n",
    "Course CSCI 5922, University of Colorado Boulder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we explore the optimization of image captioning models by combining dataset distillation and pre-trained models. In this notebok file we are focusing on Resnet50 which will serve as our baseline. We aim to reduce the\n",
    "computational burden and improve model performance, particularly in\n",
    "resource-constrained environments. Our approach involves creating dis-\n",
    "tilled datasets of different sizes (25%,50%,75% and 100%) using gradient-\n",
    "based distillation and random selection methods. We then fine-tune the\n",
    "ResNet-50 model to generate captions through an LSTM network. Per-\n",
    "formance is evaluated using metrics such as BLEU and CIDEr scores,\n",
    "as well as training time. The results will help understand the trade-off\n",
    "between dataset size, distillation techniques, and training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup, Device Configuration\n",
    "\n",
    "This cell checks if CUDA (GPU support) is available on the system. It helps verify if the model can leverage GPU acceleration for training and inference. The system selects cuda if available, otherwise defaults to CPU. \n",
    "\n",
    "Also Installation of required libraries take place as per requirement. Later in the notebook as well we install some libraries based on requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-TSPYsSykCf",
    "outputId": "d9b168b5-2e06-4a36-ed2b-7996afa38666"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Directory for Dataset and Models\n",
    "Sets up necessary folders in the required environment for organizing dataset and model files. Certain code lines are hence commented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LCwA2ge341m",
    "outputId": "0a0b18cf-fe12-48af-fd54-edb171ad98ea"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fM2OaKKq4Y6W"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# !mkdir -p /content/drive/MyDrive/deepreel/data\n",
    "# !mkdir -p /content/drive/MyDrive/deepreel/models\n",
    "# codes/DeepReel_Making_Images_Talk (1).ipynb\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.makedirs(r\"C:\\Users\\agnibdeepreel\\data\", exist_ok=True)\n",
    "# os.makedirs(r\"C:\\Users\\agnib\\deepreel\\models\", exist_ok=True)\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(r\"deepreel/data\", exist_ok=True)\n",
    "os.makedirs(r\"deepreel/models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Extract Dataset\n",
    "Downloads the COCO validation and annotation data into a specified folder if they are not already present. This serves as the dataset on which we are focusing our research. The validation dataset provides data of size 5000. The dataset is then extracted in the required folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAGbKX175WB0",
    "outputId": "fb31e5a2-3d68-4fde-e23c-e86a6471c65c"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "data_dir = r\"deepreel/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "val_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "val_zip_path = os.path.join(data_dir, \"val2017.zip\")\n",
    "urllib.request.urlretrieve(val_url, val_zip_path)\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "\n",
    "ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "ann_zip_path = os.path.join(data_dir, \"annotations_trainval2017.zip\")\n",
    "urllib.request.urlretrieve(ann_url, ann_zip_path)\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(ann_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNT9OSnd57Nr"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "val_zip = r\"deepreel/data/val2017.zip\"\n",
    "ann_zip = r\"deepreel/data/annotations_trainval2017.zip\"\n",
    "\n",
    "\n",
    "coco_path = r\"coco\"\n",
    "os.makedirs(coco_path, exist_ok=True)\n",
    "\n",
    "\n",
    "val_extract_path = os.path.join(coco_path, \"val2017\")\n",
    "if not os.path.exists(val_extract_path):\n",
    "    with zipfile.ZipFile(val_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(coco_path)\n",
    "    print(\"val2017 extracted.\")\n",
    "else:\n",
    "    print(\"val2017 already extracted.\")\n",
    "\n",
    "\n",
    "ann_extract_path = os.path.join(coco_path, \"annotations\")\n",
    "if not os.path.exists(ann_extract_path):\n",
    "    with zipfile.ZipFile(ann_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(coco_path)\n",
    "    print(\"annotations extracted.\")\n",
    "else:\n",
    "    print(\"annotations already extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ICiJej76DCu"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "!pip install -q pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s read the captions_val2017.json and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1rQunuw6Gor",
    "outputId": "baa9ef6c-f1b9-44d6-eda2-53eb0cde9877"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import json\n",
    "\n",
    "annotations_path = r\"coco/annotations/captions_val2017.json\"\n",
    "\n",
    "\n",
    "with open(annotations_path, 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "\n",
    "print(captions_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping from image_id → captions : Each image ID has 5 captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njBJS_PO6O_G",
    "outputId": "2244c7e8-eb5d-4a3c-c12a-5ef1f57d5b6f"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from collections import defaultdict\n",
    "\n",
    "# Map image_id to list of captions\n",
    "image_id_to_captions = defaultdict(list)\n",
    "\n",
    "for ann in captions_data['annotations']:\n",
    "    image_id_to_captions[ann['image_id']].append(ann['caption'])\n",
    "\n",
    "\n",
    "example_id = captions_data['annotations'][0]['image_id']\n",
    "print(f\"Image ID: {example_id}\")\n",
    "print(\"Captions:\", image_id_to_captions[example_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess images for ResNet-50\n",
    "ResNet-50 expects images to be:\n",
    "\n",
    "Size: 224x224\n",
    "\n",
    "Normalized with ImageNet stats\n",
    "\n",
    "We'll use torchvision for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxzRniAn6p0o"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Creating Custom Dataset\n",
    "\n",
    "Create a custom dataset for val2017 images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B1-WKqB665y"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_dict, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.transform = transform\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.image_dir, f'{image_id:012}.jpg')  # zero-padded filenames\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        captions = self.captions_dict[image_id]\n",
    "        return image, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSg4y6EM7CRG",
    "outputId": "305ecdc4-0bb3-4eab-f6bb-d4dd3e916552"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = CocoDataset('coco/val2017/', image_id_to_captions, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "images, captions = next(iter(dataloader))\n",
    "print(\"Batch image shape:\", images.shape)\n",
    "print(\"Captions for first image:\\n\", captions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "\n",
    "# image = images[0]\n",
    "\n",
    "\n",
    "# unnorm = lambda t: t * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "# image = unnorm(image)\n",
    "# image = torch.clamp(image, 0, 1)\n",
    "\n",
    "\n",
    "# to_pil_image(image).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from images using ResNet-50\n",
    "We will:\n",
    "\n",
    "Use a pre-trained ResNet-50 model\n",
    "\n",
    "Remove its last classification layer\n",
    "\n",
    "Get 2048-dimensional feature vectors for each image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dsb3dtrn7Uzo"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "\n",
    "modules = list(resnet.children())[:-1]  # remove last fc layer\n",
    "resnet = nn.Sequential(*modules)\n",
    "\n",
    "# Freeze the weights\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet.eval() \n",
    "resnet = resnet.cuda() if torch.cuda.is_available() else resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9pbdywU7o6N"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with torch.no_grad():\n",
    "    \n",
    "    features = resnet(images.cuda() if torch.cuda.is_available() else images)\n",
    "    features = features.to(device)\n",
    "    features = features.view(features.size(0), -1)  # flatten from (B, 2048, 1, 1) to (B, 2048)\n",
    "\n",
    "print(\"Image features shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAUNS3w47w5K"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "with open(r'deepreel/data/annotations/captions_val2017.json', 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "# Create a mapping from image_id to list of caption\n",
    "image_id_to_captions = defaultdict(list)\n",
    "for annot in captions_data['annotations']:\n",
    "    img_id = annot['image_id']\n",
    "    caption = annot['caption']\n",
    "    image_id_to_captions[img_id].append(caption)\n",
    "\n",
    "\n",
    "sample_img_id = list(image_id_to_captions.keys())[0]\n",
    "print(f\"Sample image ID: {sample_img_id}\")\n",
    "print(\"Captions:\", image_id_to_captions[sample_img_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Feature Extraction using ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n16F8ESU8Vko"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "\n",
    "\n",
    "all_captions = []\n",
    "for caps in image_id_to_captions.values():\n",
    "    for c in caps:\n",
    "        # Add start and end tokens\n",
    "        cleaned = '<start> ' + c.lower().strip() + ' <end>'\n",
    "        all_captions.append(cleaned)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# Convert captions to sequences of integer\n",
    "caption_seqs = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "# Add padding so all captions are same length\n",
    "# max_caption_len = max(len(seq) for seq in caption_seqs)\n",
    "max_caption_len = 32\n",
    "caption_seqs_padded = pad_sequences(caption_seqs, maxlen=max_caption_len, padding='post')\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding\n",
    "\n",
    "print(f\"Total captions: {len(all_captions)}\")\n",
    "print(f\"Max caption length: {max_caption_len}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(\"Sample padded sequence:\", caption_seqs_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sp4DXUT58fwS"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')  \n",
    "\n",
    "image_dir = './coco/val2017'\n",
    "\n",
    "\n",
    "image_ids = list(image_id_to_captions.keys())\n",
    "image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in image_ids]\n",
    "\n",
    "\n",
    "def load_and_preprocess_img(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "\n",
    "features_dict = {}\n",
    "for img_id, img_path in tqdm(zip(image_ids, image_paths), total=len(image_ids)):\n",
    "    try:\n",
    "        img_array = load_and_preprocess_img(img_path)\n",
    "        features = resnet_model.predict(img_array, verbose=0)\n",
    "        features_dict[img_id] = features.squeeze()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {img_path}\")\n",
    "\n",
    "\n",
    "sample_id = image_ids[0]\n",
    "print(f\"Feature shape for image {sample_id}: {features_dict[sample_id].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnVheQL6SfrU"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('features_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(features_dict, f)\n",
    "\n",
    "print(\"Features dictionary saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4EKLLrFSh4z"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('features_dict.pkl', 'rb') as f:\n",
    "    features_dict = pickle.load(f)\n",
    "\n",
    "print(\"Features dictionary loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTGQQVClSklZ"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import json\n",
    "\n",
    "\n",
    "features_dict_json = {key: value.tolist() for key, value in features_dict.items()}\n",
    "\n",
    "\n",
    "with open('features_dict.json', 'w') as f:\n",
    "    json.dump(features_dict_json, f)\n",
    "\n",
    "print(\"Features dictionary saved to JSON successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBXyaB5eSo1X"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('features_dict.json', 'r') as f:\n",
    "    features_dict_json = json.load(f)\n",
    "\n",
    "\n",
    "features_dict = {key: np.array(value) for key, value in features_dict_json.items()}\n",
    "\n",
    "print(\"Features dictionary loaded from JSON successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YO8jkH58S5QG"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if os.path.exists('features_dict.pkl'):\n",
    "    with open('features_dict.pkl', 'rb') as f:\n",
    "        features_dict = pickle.load(f)\n",
    "    print(\"Loaded existing features dictionary.\")\n",
    "else:\n",
    "    features_dict = {}\n",
    "\n",
    "\n",
    "for img_id, img_path in tqdm(zip(image_ids, image_paths), total=len(image_ids)):\n",
    "    if img_id not in features_dict:\n",
    "        try:\n",
    "            img_array = load_and_preprocess_img(img_path)\n",
    "            features = resnet_model.predict(img_array, verbose=0)\n",
    "            features_dict[img_id] = features.squeeze()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "\n",
    "\n",
    "with open('features_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4V8Xf8e_-qVC"
   },
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Embedding, LSTM, Bidirectional, Dropout, Add, Activation, Concatenate, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "\n",
    "# max_length = max(len(c.split()) for c in all_captions)\n",
    "caption_lengths = [len(c.split()) for c in all_captions]\n",
    "max_length = int(np.percentile(caption_lengths, 90))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "X1, X2, y = [], [], []\n",
    "\n",
    "for img_id, caption_list in image_id_to_captions.items():\n",
    "    feature = features_dict.get(img_id)\n",
    "    if feature is None:\n",
    "        continue\n",
    "    for caption in caption_list:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_word = seq[:i], seq[i]\n",
    "            \n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_word)\n",
    "\n",
    "\n",
    "X1 = np.array(X1, dtype='float32')\n",
    "X2 = np.array(X2)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Image features shape: {X1.shape}\")\n",
    "print(f\"Input sequence shape: {X2.shape}\")\n",
    "print(f\"Target word shape: {y.shape}\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((X1, X2), y))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        \n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = Bidirectional(LSTM(256, return_sequences=True))(se2)\n",
    "\n",
    "\n",
    "# fe2_expanded = tf.expand_dims(fe2, 1)  \n",
    "fe2_expanded = Lambda(lambda x: tf.expand_dims(x, 1))(fe2)\n",
    "attention = BahdanauAttention(256)\n",
    "context_vector, attention_weights = attention(se3, fe2)\n",
    "\n",
    "\n",
    "decoder_input = Concatenate()([context_vector, fe2])\n",
    "decoder2 = Dense(256, activation='relu')(decoder_input)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "print(\"TensorFlow using GPU(s):\", tf.config.list_physical_devices('GPU'))\n",
    "# model = model.to(device)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(1e-4), metrics=['sparse_categorical_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrM5mKvJFXZ2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmxC3sgsFYBR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS9T5VHgF4Du"
   },
   "source": [
    "# GENERATE CAPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixG1FDjEF4vt"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def generate_caption(model, image_feature, tokenizer, max_len):\n",
    "    in_text = '<start>'\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=max_len, padding='post')\n",
    "        yhat = model.predict([np.expand_dims(image_feature, axis=0), sequence], verbose=0)\n",
    "        yhat_idx = np.argmax(yhat[0])  # choose word with highest probability\n",
    "        word = tokenizer.index_word.get(yhat_idx, None)\n",
    "        if word is None or word == '<end>':\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "\n",
    "    return in_text.replace('<start>', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1z_WMShF9Pw"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sample_id = image_ids[10] \n",
    "sample_feature = features_dict[sample_id]\n",
    "# max_caption_len = 15\n",
    "max_caption_len = max_length\n",
    "\n",
    "caption = generate_caption(model, sample_feature, tokenizer, max_caption_len)\n",
    "print(f\"Generated caption:\\n{caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA DISTILLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-sGXUzRRnAj"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import random\n",
    "def random_selection(image_ids, percentage):\n",
    "    \n",
    "    num_images_to_select = int(len(image_ids) * percentage / 100)\n",
    "    selected_image_ids = random.sample(image_ids, num_images_to_select)\n",
    "    selected_image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in selected_image_ids]\n",
    "\n",
    "    return selected_image_ids, selected_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0iEHdB0Rs1j"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def gradient_based_selection(image_ids, features_dict, percentage):\n",
    "    \n",
    "    feature_vectors = np.array([features_dict[img_id] for img_id in image_ids])\n",
    "    distance_matrix = pairwise_distances(feature_vectors, metric='euclidean')\n",
    "    distance_sums = np.sum(distance_matrix, axis=1)\n",
    "    \n",
    "    num_select = int(len(image_ids) * percentage / 100)\n",
    "    selected_indices = np.argsort(distance_sums)[-num_select:]\n",
    "    selected_image_ids = [image_ids[i] for i in selected_indices]\n",
    "    selected_image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in selected_image_ids]\n",
    "    \n",
    "    return selected_image_ids, selected_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def prepare_target_caption(image_id, tokenizer, max_len=30, ground_truths=None):\n",
    "    \n",
    "    caption = ground_truths[image_id][0]\n",
    "    tokens = tokenizer.encode(caption, return_tensors='pt', padding='max_length',\n",
    "                              max_length=max_len, truncation=True)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def generate_distilled_captions(image_ids, image_paths, model, tokenizer, max_caption_len, batch_size=32):\n",
    "    \n",
    "    distilled_captions = {}\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_ids = image_ids[i:i+batch_size]\n",
    "        batch_images = np.vstack([load_and_preprocess_img(path) for path in batch_paths])\n",
    "        batch_features = resnet_model.predict(batch_images, verbose=0)\n",
    "        \n",
    "        for j, feature in enumerate(batch_features):\n",
    "            caption = generate_caption(model, feature.squeeze(), tokenizer, max_caption_len)\n",
    "            distilled_captions[batch_ids[j]] = caption\n",
    "\n",
    "    return distilled_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import time\n",
    "\n",
    "def measure_training_time(dataset_image_ids, dataset_image_paths, model, tokenizer, max_caption_len):\n",
    "    start_time = time.time()\n",
    "    generate_distilled_captions(dataset_image_ids, dataset_image_paths, model, tokenizer, max_caption_len)\n",
    "    return time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1kicQ_yRxf7"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Distill dataset using random selection\n",
    "random_25_ids, random_25_paths = random_selection(image_ids, 25)\n",
    "random_50_ids, random_50_paths = random_selection(image_ids, 50)\n",
    "random_75_ids, random_75_paths = random_selection(image_ids, 75)\n",
    "random_100_ids, random_100_paths = random_selection(image_ids, 100)\n",
    "\n",
    "# Distill dataset using gradient-based selection (with mock method)\n",
    "gradient_25_ids, gradient_25_paths = gradient_based_selection(image_ids, features_dict, 25)\n",
    "gradient_50_ids, gradient_50_paths = gradient_based_selection(image_ids, features_dict, 50)\n",
    "gradient_75_ids, gradient_75_paths = gradient_based_selection(image_ids, features_dict, 75)\n",
    "gradient_100_ids, gradient_100_paths = gradient_based_selection(image_ids, features_dict, 100)\n",
    "\n",
    "# Generate captions for each distillation size\n",
    "captions_random_25 = generate_distilled_captions(random_25_ids, random_25_paths, model, tokenizer, max_caption_len)\n",
    "captions_random_50 = generate_distilled_captions(random_50_ids, random_50_paths, model, tokenizer, max_caption_len)\n",
    "captions_random_75 = generate_distilled_captions(random_75_ids, random_75_paths, model, tokenizer, max_caption_len)\n",
    "captions_random_100 = generate_distilled_captions(random_100_ids, random_100_paths, model, tokenizer, max_caption_len)\n",
    "\n",
    "captions_gradient_25 = generate_distilled_captions(gradient_25_ids, gradient_25_paths, model, tokenizer, max_caption_len)\n",
    "captions_gradient_50 = generate_distilled_captions(gradient_50_ids, gradient_50_paths, model, tokenizer, max_caption_len)\n",
    "captions_gradient_75 = generate_distilled_captions(gradient_75_ids, gradient_75_paths, model, tokenizer, max_caption_len)\n",
    "captions_gradient_100 = generate_distilled_captions(gradient_100_ids, gradient_100_paths, model, tokenizer, max_caption_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A83XhlKR8Qu"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "time_random_25 = measure_training_time(random_25_ids, random_25_paths, model, tokenizer, max_caption_len)\n",
    "time_random_50 = measure_training_time(random_50_ids, random_50_paths, model, tokenizer, max_caption_len)\n",
    "time_random_75 = measure_training_time(random_75_ids, random_75_paths, model, tokenizer, max_caption_len)\n",
    "time_random_100 = measure_training_time(random_100_ids, random_100_paths, model, tokenizer, max_caption_len)\n",
    "\n",
    "time_gradient_25 = measure_training_time(gradient_25_ids, gradient_25_paths, model, tokenizer, max_caption_len)\n",
    "time_gradient_50 = measure_training_time(gradient_50_ids, gradient_50_paths, model, tokenizer, max_caption_len)\n",
    "time_gradient_75 = measure_training_time(gradient_75_ids, gradient_75_paths, model, tokenizer, max_caption_len)\n",
    "time_gradient_100 = measure_training_time(gradient_100_ids, gradient_100_paths, model, tokenizer, max_caption_len)\n",
    "\n",
    "# Print training times\n",
    "print(f\"Random 25% Time: {time_random_25}s\")\n",
    "print(f\"Random 50% Time: {time_random_50}s\")\n",
    "print(f\"Random 75% Time: {time_random_75}s\")\n",
    "print(f\"Random 100% Time: {time_random_100}s\")\n",
    "\n",
    "print(f\"Gradient 25% Time: {time_gradient_25}s\")\n",
    "print(f\"Gradient 50% Time: {time_gradient_50}s\")\n",
    "print(f\"Gradient 75% Time: {time_gradient_75}s\")\n",
    "print(f\"Gradient 100% Time: {time_gradient_100}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "annotation_file = r\"deepreel/data/annotations/captions_val2017.json\"\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "ground_truth_captions = {}\n",
    "\n",
    "for img_id in coco.getImgIds():\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    ground_truth_captions[img_id] = [ann['caption'] for ann in anns]\n",
    "\n",
    "print(f\"Total images with ground truth captions: {len(ground_truth_captions)}\")\n",
    "print(\"Sample:\\n\", list(ground_truth_captions.items())[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION OF CAPTIONS BY BLEU AND CIDEr Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "annotation_file = \"deepreel/data/annotations/captions_val2017.json\"\n",
    "coco_gt = COCO(annotation_file)\n",
    "\n",
    "\n",
    "def evaluate_captions(coco_gt, generated_captions, exclude_metrics):\n",
    "    \n",
    "    valid_img_ids = set(coco_gt.getImgIds())\n",
    "\n",
    "    filtered_generated = {\n",
    "        img_id: caption for img_id, caption in generated_captions.items()\n",
    "        if img_id in valid_img_ids\n",
    "    }\n",
    "\n",
    "    if not filtered_generated:\n",
    "        raise ValueError(\"No valid image IDs found in generated captions.\")\n",
    "\n",
    "    results = [{\"image_id\": img_id, \"caption\": filtered_generated[img_id]}\n",
    "               for img_id in sorted(filtered_generated.keys())]\n",
    "\n",
    "    coco_res = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOEvalCap(coco_gt, coco_res)\n",
    "    coco_eval.params['image_id'] = list(filtered_generated.keys())\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    scores = coco_eval.eval\n",
    "    if exclude_metrics:\n",
    "        scores = {k: v for k, v in scores.items() if k not in exclude_metrics}\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_25 = evaluate_captions(coco_gt, captions_random_25, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\"Evaluation Metrics for Random 25%:\")\n",
    "for metric, score in scores_random_25.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_50 = evaluate_captions(coco_gt, captions_random_50, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\"Evaluation Metrics for Random 50%:\")\n",
    "for metric, score in scores_random_50.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_75 = evaluate_captions(coco_gt, captions_random_75, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\"Evaluation Metrics for Random 75% (SPICE excluded):\")\n",
    "for metric, score in scores_random_75.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_100 = evaluate_captions(coco_gt, captions_random_100, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\"Evaluation Metrics for Random 25%:\")\n",
    "for metric, score in scores_random_100.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gradient_25 = evaluate_captions(coco_gt, captions_gradient_25, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for Gradient 25%:\")\n",
    "for metric, score in scores_gradient_25.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gradient_50 = evaluate_captions(coco_gt, captions_gradient_50, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\"Evaluation Metrics for gradient 50%\")\n",
    "for metric, score in scores_gradient_50.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrB9t3RfR2UO"
   },
   "outputs": [],
   "source": [
    "scores_gradient_75 = evaluate_captions(coco_gt, captions_gradient_75, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\"Evaluation Metrics for gradient 75%\")\n",
    "for metric, score in scores_gradient_75.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gradient_100 = evaluate_captions(coco_gt, captions_gradient_100, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\"Evaluation Metrics for gradient 100%\")\n",
    "for metric, score in scores_gradient_100.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUAL TESTING OF GENERATED CAPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show image\n",
    "# image_path = \"\"\n",
    "# try:\n",
    "#     img = Image.open(image_path)\n",
    "#     plt.imshow(img)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(caption)\n",
    "#     plt.show()\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Image {image_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_image_captioning():\n",
    "    image_ids = [\"000000000139\", \"000000000285\", \"000000000632\"]\n",
    "    image_ids = [int(img_id) for img_id in image_ids] \n",
    "\n",
    "    for img_id in image_ids:\n",
    "        feature = features_dict.get(img_id)\n",
    "        print(feature)\n",
    "        if feature is None:\n",
    "            continue\n",
    "        caption = generate_caption(model, feature, tokenizer, max_length)\n",
    "        print(caption)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_random_image_captioning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
