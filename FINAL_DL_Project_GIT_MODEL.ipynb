{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f738057-0d64-4776-a495-cf290404be42",
   "metadata": {},
   "source": [
    "# Impact of Dataset Size and Distillation Techniques on Image Captioning Performance: An Empirical Study\n",
    "\n",
    "Authors: Srushti Sangawar and Arunava Ghosh\n",
    "\n",
    "Course CSCI 5922, University of Colorado Boulder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d2293-fdb8-4226-89cb-ad364e936a68",
   "metadata": {},
   "source": [
    "In this project, we explore the optimization of image captioning models by combining dataset distillation and pre-trained models. In this notebook file we are focusing on GIT to get a comparitive understanding vs our baseline model. Like in the other notebooks for the research we aim to reduce the\n",
    "computational burden and improve model performance, particularly in\n",
    "resource-constrained environments. Our approach involves creating dis-\n",
    "tilled datasets of different sizes (25%,50%,75% and 100%) using gradient-\n",
    "based distillation and random selection methods. We then fine-tune the\n",
    "GIT model as per requirement. Per-\n",
    "formance is evaluated using metrics such as BLEU and CIDEr scores,\n",
    "as well as training time. The results will help understand the trade-off\n",
    "between dataset size, distillation techniques, and training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4530d-aa1f-4dfe-8e98-1ce267733876",
   "metadata": {},
   "source": [
    "# Environment Setup, Device Configuration\n",
    "\n",
    "This cell checks if CUDA (GPU support) is available on the system. It helps verify if the model can leverage GPU acceleration for training and inference. The system selects cuda if available, otherwise defaults to CPU. \n",
    "\n",
    "Also Installation of required libraries take place as per requirement. Later in the notebook as well we install some libraries based on requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3da0d-bdbb-449f-a7f3-4733867d3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cacb146-8303-461f-8515-d60ed544802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc344a8-d458-4350-8f0d-bd50a7b40878",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Check for GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c808e-2360-4513-be8b-f530c211729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61caa1c-4c00-4d6a-ab31-6bf620de2050",
   "metadata": {},
   "source": [
    "# Create Directory for Dataset and Models\n",
    "Sets up necessary folders in the required environment for organizing dataset and model files. Certain code lines are hence commented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7abdc-d597-4536-921c-3b5600b664dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# !mkdir -p /content/drive/MyDrive/deepreel/data\n",
    "# !mkdir -p /content/drive/MyDrive/deepreel/models\n",
    "# codes/DeepReel_Making_Images_Talk (1).ipynb\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.makedirs(r\"C:\\Users\\agnibdeepreel\\data\", exist_ok=True)\n",
    "# os.makedirs(r\"C:\\Users\\agnib\\deepreel\\models\", exist_ok=True)\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(r\"deepreel/data\", exist_ok=True)\n",
    "os.makedirs(r\"deepreel/models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca846d4-0e78-4bc4-ab1d-a30bc1e4ff17",
   "metadata": {},
   "source": [
    "# Download and Extract Dataset\n",
    "Downloads the COCO validation and annotation data into a specified folder if they are not already present. This serves as the dataset on which we are focusing our research. The validation dataset provides data of size 5000. The dataset is then extracted in the required folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d8d11-48a5-4a1a-8434-302ee4fe2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create destination directory\n",
    "data_dir = r\"deepreel/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download val2017.zip\n",
    "val_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "val_zip_path = os.path.join(data_dir, \"val2017.zip\")\n",
    "urllib.request.urlretrieve(val_url, val_zip_path)\n",
    "\n",
    "# Extract val2017.zip\n",
    "with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "# Download annotations_trainval2017.zip\n",
    "ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "ann_zip_path = os.path.join(data_dir, \"annotations_trainval2017.zip\")\n",
    "urllib.request.urlretrieve(ann_url, ann_zip_path)\n",
    "\n",
    "# Extract annotations zip\n",
    "with zipfile.ZipFile(ann_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45118fac-08e7-4056-b9d3-9b9494a09c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Path to ZIP files\n",
    "val_zip = r\"deepreel/data/val2017.zip\"\n",
    "ann_zip = r\"deepreel/data/annotations_trainval2017.zip\"\n",
    "\n",
    "# Target extraction path\n",
    "coco_path = r\"coco\"\n",
    "os.makedirs(coco_path, exist_ok=True)\n",
    "\n",
    "# Extract val2017.zip\n",
    "val_extract_path = os.path.join(coco_path, \"val2017\")\n",
    "if not os.path.exists(val_extract_path):\n",
    "    with zipfile.ZipFile(val_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(coco_path)\n",
    "    print(\"val2017 extracted.\")\n",
    "else:\n",
    "    print(\"val2017 already extracted.\")\n",
    "\n",
    "# Extract annotations_trainval2017.zip\n",
    "ann_extract_path = os.path.join(coco_path, \"annotations\")\n",
    "if not os.path.exists(ann_extract_path):\n",
    "    with zipfile.ZipFile(ann_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(coco_path)\n",
    "    print(\"annotations extracted.\")\n",
    "else:\n",
    "    print(\"annotations already extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786eb60-465a-4cd6-807a-ba241a77b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "!pip install -q pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83643474-fac8-4737-b2f7-ad2da93059df",
   "metadata": {},
   "source": [
    "Now let’s read the captions_val2017.json and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d495b71-5d39-4885-a7fa-ff5c7e64c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import json\n",
    "\n",
    "annotations_path = r\"coco/annotations/captions_val2017.json\"\n",
    "\n",
    "# Load annotations\n",
    "with open(annotations_path, 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "# Preview the keys\n",
    "print(captions_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b073f61-810f-486a-9cb4-29d20f47332d",
   "metadata": {},
   "source": [
    "Create a mapping from image_id → captions : Each image ID has 5 captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea93e2d-6b50-4797-9e62-7f7ca3750c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from collections import defaultdict\n",
    "\n",
    "# Map image_id to list of captions\n",
    "image_id_to_captions = defaultdict(list)\n",
    "\n",
    "for ann in captions_data['annotations']:\n",
    "    image_id_to_captions[ann['image_id']].append(ann['caption'])\n",
    "\n",
    "# Show one example\n",
    "example_id = captions_data['annotations'][0]['image_id']\n",
    "print(f\"Image ID: {example_id}\")\n",
    "print(\"Captions:\", image_id_to_captions[example_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db51cc-5201-46ab-bacc-6a0b9f33dcac",
   "metadata": {},
   "source": [
    "# Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df58b30-9760-46ee-937b-789c384f3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cb72e-2aac-4048-bcfd-2fb8d4ac9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/git-base\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d8cc9-91f5-4f39-bbb1-ba006f7fead5",
   "metadata": {},
   "source": [
    "#  Creating Custom Dataset\n",
    "\n",
    "Create a custom dataset for val2017 images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b65a8-583c-452d-8a67-cc3c0b64d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_dict, processor):\n",
    "        self.image_dir = image_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.processor = processor\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.image_dir, f'{image_id:012}.jpg')\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        caption = self.captions_dict[image_id][0]\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            legacy=False\n",
    "        )\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"]\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9866555-4d85-4a8c-a428-1402af7e42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CocoDataset(\n",
    "    image_dir=\"coco/val2017\",\n",
    "    captions_dict=image_id_to_captions,\n",
    "    processor=processor\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b46232-834b-429c-a823-3ffc9beb8edc",
   "metadata": {},
   "source": [
    "# Model initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfb8ce-f34f-4d4b-ab97-3ffb244c80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a986a-6858-4f8d-bc92-a51f12ecbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(pixel_values=inputs[\"pixel_values\"])\n",
    "    return processor.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffede534-ee17-4df1-bdd6-9c72367852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_path = \"coco/val2017/000000000139.jpg\"\n",
    "print(\"Generated Caption:\", generate_caption(sample_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051100-5dc6-426e-9d4f-4221449bd616",
   "metadata": {},
   "source": [
    "# DATA DISTILLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c76610-9dae-4f2e-b0be-214c47067474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# import random\n",
    "# def random_selection(image_ids, percentage):\n",
    "#     \"\"\"\n",
    "#     Select a subset of images based on random selection.\n",
    "\n",
    "#     Arguments:\n",
    "#     - image_ids: List of image IDs from the dataset.\n",
    "#     - percentage: Percentage of dataset to be selected.\n",
    "\n",
    "#     Returns:\n",
    "#     - selected_image_ids: Subset of image IDs.\n",
    "#     - selected_image_paths: Corresponding image file paths.\n",
    "#     \"\"\"\n",
    "#     num_images_to_select = int(len(image_ids) * percentage / 100)\n",
    "#     selected_image_ids = random.sample(image_ids, num_images_to_select)\n",
    "#     selected_image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in selected_image_ids]\n",
    "\n",
    "#     return selected_image_ids, selected_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c369a46-283a-4077-8fc0-df0ad16a9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f146280-4d7c-4a69-b454-fb9ec8d99a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# def gradient_based_selection(image_ids, features_dict, percentage):\n",
    "#     '''\n",
    "#     Optimized selection using pairwise distances (O(n^2), but fast with vectorized ops)\n",
    "#     '''\n",
    "#     feature_vectors = np.array([features_dict[img_id] for img_id in image_ids])\n",
    "#     distance_matrix = pairwise_distances(feature_vectors, metric='euclidean')\n",
    "#     distance_sums = np.sum(distance_matrix, axis=1)\n",
    "    \n",
    "#     num_select = int(len(image_ids) * percentage / 100)\n",
    "#     selected_indices = np.argsort(distance_sums)[-num_select:]\n",
    "#     selected_image_ids = [image_ids[i] for i in selected_indices]\n",
    "#     selected_image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in selected_image_ids]\n",
    "    \n",
    "#     return selected_image_ids, selected_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195cb8d9-324a-44fa-9556-62571d7025e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# def generate_distilled_captions(image_ids, image_paths, model, tokenizer, max_caption_len, batch_size=32):\n",
    "#     '''\n",
    "#     Generate captions in batches for better performance.\n",
    "#     '''\n",
    "#     distilled_captions = {}\n",
    "    \n",
    "#     for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "#         batch_paths = image_paths[i:i+batch_size]\n",
    "#         batch_ids = image_ids[i:i+batch_size]\n",
    "#         batch_images = np.vstack([load_and_preprocess_img(path) for path in batch_paths])\n",
    "#         batch_features = resnet_model.predict(batch_images, verbose=0)\n",
    "        \n",
    "#         for j, feature in enumerate(batch_features):\n",
    "#             caption = generate_caption(model, feature.squeeze(), tokenizer, max_caption_len)\n",
    "#             distilled_captions[batch_ids[j]] = caption\n",
    "\n",
    "#     return distilled_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e00577-5735-4f4d-a0e7-81d6a66d8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc18941-9196-43df-97a5-aebc156b7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_training_time(dataset_image_ids, dataset_image_paths, model, tokenizer, max_caption_len):\n",
    "    start_time = time.time()\n",
    "    generate_distilled_captions(dataset_image_ids, dataset_image_paths, model, tokenizer, max_caption_len)\n",
    "    return time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c04ff4-b500-4852-9455-aede3c7895ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './coco/val2017'\n",
    "\n",
    "# Pick only image files for which we have captions\n",
    "image_ids = list(image_id_to_captions.keys())\n",
    "image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7febc33-7fef-4fc6-a0e8-f708ff729984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "\n",
    "# def random_selection(image_ids, image_dir, percentage):\n",
    "#     \"\"\"\n",
    "#     Select a subset of images based on random selection.\n",
    "\n",
    "#     Arguments:\n",
    "#     - image_ids: List of image IDs from the dataset.\n",
    "#     - image_dir: Directory where images are stored.\n",
    "#     - percentage: Percentage of dataset to be selected.\n",
    "\n",
    "#     Returns:\n",
    "#     - selected_image_ids: Subset of image IDs.\n",
    "#     - selected_image_paths: Corresponding image file paths.\n",
    "#     \"\"\"\n",
    "#     num_images_to_select = int(len(image_ids) * percentage / 100)\n",
    "#     selected_image_ids = random.sample(image_ids, num_images_to_select)\n",
    "#     selected_image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in selected_image_ids]\n",
    "\n",
    "#     return selected_image_ids, selected_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4246b5e-6edd-4a49-85b6-910740fc7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_25_ids, random_25_paths = random_selection(image_ids, image_dir,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2978b-1c2b-4e24-a559-d885ad00e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08dcf56-3c11-4205-baf8-3a63e320a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "\n",
    "# Gather all captions into one list\n",
    "all_captions = []\n",
    "for caps in image_id_to_captions.values():\n",
    "    for c in caps:\n",
    "        # Add start and end tokens\n",
    "        cleaned = '<start> ' + c.lower().strip() + ' <end>'\n",
    "        all_captions.append(cleaned)\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# Convert captions to sequences of integers\n",
    "caption_seqs = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "# Add padding so all captions are same length\n",
    "max_caption_len = max(len(seq) for seq in caption_seqs)\n",
    "caption_seqs_padded = pad_sequences(caption_seqs, maxlen=max_caption_len, padding='post')\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding\n",
    "\n",
    "print(f\"Total captions: {len(all_captions)}\")\n",
    "print(f\"Max caption length: {max_caption_len}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(\"Sample padded sequence:\", caption_seqs_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9621014-dcac-4c07-b97c-a34750d500b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_random_25 = generate_distilled_captions(random_25_ids, random_25_paths, model, tokenizer, max_caption_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fc14d-88c0-4a12-b9bb-63b01c1a9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_distilled_captions(image_ids, image_paths, model, processor, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate distilled captions directly using the trained GIT model.\n",
    "    \"\"\"\n",
    "    distilled_captions = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_ids = image_ids[i:i+batch_size]\n",
    "        batch_images = [Image.open(path).convert('RGB') for path in batch_paths]\n",
    "\n",
    "        # Preprocess\n",
    "        inputs = processor(images=batch_images, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(pixel_values=inputs[\"pixel_values\"], max_length=50)\n",
    "\n",
    "        # Decode\n",
    "        decoded_captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for img_id, caption in zip(batch_ids, decoded_captions):\n",
    "            distilled_captions[img_id] = caption.strip()\n",
    "\n",
    "    return distilled_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bc35d-26ae-483f-87b4-d30e469e683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def random_selection(image_ids, image_dir, percentage):\n",
    "    \"\"\"\n",
    "    Randomly select a subset of images.\n",
    "    \"\"\"\n",
    "    num_images_to_select = int(len(image_ids) * percentage / 100)\n",
    "    selected_image_ids = random.sample(image_ids, num_images_to_select)\n",
    "    selected_image_paths = [os.path.join(image_dir, f\"{image_id:012}.jpg\") for image_id in selected_image_ids]\n",
    "\n",
    "    return selected_image_ids, selected_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd0bd5-8396-4864-8254-4cfc2cafc199",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids_25, random_paths_25 = random_selection(image_ids, image_dir, percentage=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefb4c0-7620-4442-8081-fedf6624b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_captions_random_25 = generate_distilled_captions(random_ids_25, random_paths_25, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a67c34-3fea-4771-84d0-922aa2e3f16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f6ce2-dae7-479c-94b1-35f206f39de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your save directory\n",
    "save_directory = \"deepreel/models/git_finetuned\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save processor (tokenizer + feature extractor)\n",
    "processor.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and processor saved successfully at: {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddcddc-529b-4983-b936-7c6325c89424",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids_50, random_paths_50 = random_selection(image_ids, image_dir, percentage=50)\n",
    "distilled_captions_random_50 = generate_distilled_captions(random_ids_50, random_paths_50, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f829d94-46b4-495f-b30d-f507141df914",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids_75, random_paths_75 = random_selection(image_ids, image_dir, percentage=75)\n",
    "distilled_captions_random_75 = generate_distilled_captions(random_ids_75, random_paths_75, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e696955-2314-4421-8054-73d4a32df9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids_100, random_paths_100 = random_selection(image_ids, image_dir, percentage=100)\n",
    "distilled_captions_random_100 = generate_distilled_captions(random_ids_100, random_paths_100, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd563e8-d55d-4312-8d73-3830de008ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CLIP model for feature extraction\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "def extract_clip_features(image_dir, image_ids):\n",
    "    features_dict = {}\n",
    "    \n",
    "    for image_id in tqdm(image_ids):\n",
    "        image_path = os.path.join(image_dir, f\"{image_id:012}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Preprocess\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "        # Normalize features (important for distance calculation later)\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        image_features = image_features.squeeze().cpu().numpy()\n",
    "\n",
    "        features_dict[image_id] = image_features\n",
    "\n",
    "    return features_dict\n",
    "\n",
    "# Usage\n",
    "image_dir = \"\"  # image folder path added here\n",
    "features_dict = extract_clip_features(image_dir, image_ids)\n",
    "\n",
    "print(f\"Extracted features for {len(features_dict)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a76de-c271-47b2-9e83-5b69013845b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import os\n",
    "\n",
    "def gradient_based_selection(image_ids, features_dict, image_dir, percentage):\n",
    "    feature_vectors = np.array([features_dict[img_id] for img_id in image_ids])\n",
    "    distance_matrix = pairwise_distances(feature_vectors, metric='euclidean')\n",
    "    distance_sums = np.sum(distance_matrix, axis=1)\n",
    "\n",
    "    num_select = int(len(image_ids) * percentage / 100)\n",
    "    selected_indices = np.argsort(distance_sums)[-num_select:]\n",
    "\n",
    "    selected_image_ids = [image_ids[i] for i in selected_indices]\n",
    "    selected_image_paths = [os.path.join(image_dir, f\"{image_ids[i]:012}.jpg\") for i in selected_indices]\n",
    "\n",
    "    return selected_image_ids, selected_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa209e-2c49-4643-8545-fa7fb5589fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_ids_25, gradient_paths_25 = gradient_based_selection(image_ids, features_dict,image_dir, percentage=25)\n",
    "distilled_captions_gradient_25 = generate_distilled_captions(gradient_ids_25, gradient_paths_25, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f2ade-ccb1-4225-b4ee-717836789a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_ids_50, gradient_paths_50 = gradient_based_selection(image_ids, features_dict,image_dir, percentage=50)\n",
    "distilled_captions_gradient_50 = generate_distilled_captions(gradient_ids_50, gradient_paths_50, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da4c31-d1d6-483b-a719-afa50a1c3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_ids_75, gradient_paths_75 = gradient_based_selection(image_ids, features_dict,image_dir, percentage=75)\n",
    "distilled_captions_gradient_75 = generate_distilled_captions(gradient_ids_75, gradient_paths_75, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5f415-c1eb-489c-80cb-a0c18e115f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_ids_100, gradient_paths_100 = gradient_based_selection(image_ids, features_dict,image_dir, percentage=100)\n",
    "distilled_captions_gradient_100 = generate_distilled_captions(gradient_ids_100, gradient_paths_100, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eed61a-482f-485e-986c-bdd0e7ee42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Path to COCO annotations (adjust if needed)\n",
    "annotation_file = r\"deepreel/data/annotations/captions_val2017.json\"\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Create dictionary mapping image_id to a list of reference captions\n",
    "ground_truth_captions = {}\n",
    "\n",
    "for img_id in coco.getImgIds():\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    ground_truth_captions[img_id] = [ann['caption'] for ann in anns]\n",
    "\n",
    "print(f\"Total images with ground truth captions: {len(ground_truth_captions)}\")\n",
    "print(\"Sample:\\n\", list(ground_truth_captions.items())[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df67aa-7a7c-491c-a83e-307f3b2904da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "annotation_file = \"deepreel/data/annotations/captions_val2017.json\"\n",
    "coco_gt = COCO(annotation_file)\n",
    "\n",
    "\n",
    "def evaluate_captions(coco_gt, generated_captions, exclude_metrics):\n",
    "    \n",
    "    valid_img_ids = set(coco_gt.getImgIds())\n",
    "\n",
    "    filtered_generated = {\n",
    "        img_id: caption for img_id, caption in generated_captions.items()\n",
    "        if img_id in valid_img_ids\n",
    "    }\n",
    "\n",
    "    if not filtered_generated:\n",
    "        raise ValueError(\"No valid image IDs found in generated captions.\")\n",
    "\n",
    "    results = [{\"image_id\": img_id, \"caption\": filtered_generated[img_id]}\n",
    "               for img_id in sorted(filtered_generated.keys())]\n",
    "\n",
    "    coco_res = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOEvalCap(coco_gt, coco_res)\n",
    "    coco_eval.params['image_id'] = list(filtered_generated.keys())\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    # Exclude metrics if specified\n",
    "    scores = coco_eval.eval\n",
    "    if exclude_metrics:\n",
    "        scores = {k: v for k, v in scores.items() if k not in exclude_metrics}\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores_random_25 = evaluate_captions(coco_gt, distilled_captions_random_25, exclude_metrics=[\"SPICE\"])\n",
    "# scores_gradient_25 = evaluate_captions(coco_gt, captions_gradient_25, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for Random 25%:\")\n",
    "for metric, score in scores_random_25.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d6b8a-576e-4a2e-93b8-0a74a6fe9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_50 = evaluate_captions(coco_gt, distilled_captions_random_50, exclude_metrics=[\"SPICE\"])\n",
    "# scores_gradient_25 = evaluate_captions(coco_gt, captions_gradient_25, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for Random 50% :\")\n",
    "for metric, score in scores_random_50.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d1284-f9ee-4d74-90e8-e9e36f0fd6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_75 = evaluate_captions(coco_gt, distilled_captions_random_75, exclude_metrics=[\"SPICE\"])\n",
    "# scores_gradient_25 = evaluate_captions(coco_gt, captions_gradient_25, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for Random 75% :\")\n",
    "for metric, score in scores_random_75.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661e931-b871-45d4-9000-b5dd89642743",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_100 = evaluate_captions(coco_gt, distilled_captions_random_100, exclude_metrics=[\"SPICE\"])\n",
    "# scores_gradient_25 = evaluate_captions(coco_gt, captions_gradient_25, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for Random 100% :\")\n",
    "for metric, score in scores_random_100.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded83cd-592a-44fe-889d-acb43936dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gradient_100 = evaluate_captions(coco_gt, distilled_captions_gradient_100, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for gradient 100% :\")\n",
    "for metric, score in scores_gradient_100.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b39370-9f84-4f0d-99b0-d23b7440d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gradient_75 = evaluate_captions(coco_gt, distilled_captions_gradient_75, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for gradient 75% :\")\n",
    "for metric, score in scores_gradient_75.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21a9b8-bc48-4487-a9ed-940e36ac1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gradient_50 = evaluate_captions(coco_gt, distilled_captions_gradient_50, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for gradient 50% :\")\n",
    "for metric, score in scores_gradient_50.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e171a1-2a6d-4b94-b808-467610398d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gradient_25 = evaluate_captions(coco_gt, distilled_captions_gradient_25, exclude_metrics=[\"SPICE\"])\n",
    "\n",
    "print(\" Evaluation Metrics for gradient 25% :\")\n",
    "for metric, score in scores_gradient_25.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4be8c-f936-480e-9c63-867a7df6b484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
